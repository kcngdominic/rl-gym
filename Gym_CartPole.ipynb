{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPC1l9LTkqgtT2HLPhR+Iv/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcngdominic/rl-gym/blob/main/Gym_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQd5izQe2QW7",
        "outputId": "cad257da-38ef-435c-86fc-55da0ded10ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pdb\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network model\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "0Z4J9_Nk4aHQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the replay buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, transition):\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(transition)\n",
        "        else:\n",
        "            self.memory[self.position] = transition\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "5yUjSE-V4b8T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transition:\n",
        "    def __init__(self, state, action, next_state, reward, done):\n",
        "        self.state = state\n",
        "        self.action = action\n",
        "        self.next_state = next_state\n",
        "        self.reward = reward\n",
        "        self.done = done\n"
      ],
      "metadata": {
        "id": "Fofda2VA5CKY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize environment and hyperparameters\n",
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "epsilon_decay = 0.995\n",
        "gamma = 0.99\n",
        "learning_rate = 0.001\n",
        "target_update = 10\n",
        "buffer_capacity = 10000\n",
        "batch_size = 64\n",
        "\n",
        "# Initialize model, optimizer, and replay buffer\n",
        "policy_net = QNetwork(state_size, action_size)\n",
        "target_net = QNetwork(state_size, action_size)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "replay_buffer = ReplayBuffer(buffer_capacity)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qyFZMpB4dpI",
        "outputId": "9d8c88e3-8e71-48a7-9710-d6fe378f1ab3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define exploration-exploitation strategy (epsilon-greedy)\n",
        "def select_action(state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return env.action_space.sample()  # Exploration\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state).max(1)[1].view(1, 1).item()  # Exploitation\n",
        "\n"
      ],
      "metadata": {
        "id": "fNdduY7A4fxr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_episodes = 1000\n",
        "epsilon = 1.0\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
        "\n",
        "    total_reward = 0\n",
        "    while True:\n",
        "        # Select and perform an action\n",
        "        action = select_action(state, epsilon)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float32).view(1, -1)\n",
        "        reward = torch.tensor([reward], dtype=torch.float32)\n",
        "\n",
        "        # Store the transition in the replay buffer\n",
        "        replay_buffer.push((state, action, next_state, reward, done))\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "        total_reward += reward.item()\n",
        "\n",
        "        # Sample and optimize the model\n",
        "        if len(replay_buffer.memory) > batch_size:\n",
        "            transitions = replay_buffer.sample(batch_size)\n",
        "            batch = Transition(*zip(*transitions))\n",
        "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), dtype=torch.bool)\n",
        "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "            state_batch = torch.cat(batch.state)\n",
        "            action_batch = torch.cat(batch.action)\n",
        "            reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "            # Compute Q values\n",
        "            current_q_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "            # Compute target Q values\n",
        "            next_q_values = torch.zeros(batch_size)\n",
        "            next_q_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "            target_q_values = (next_q_values * gamma) + reward_batch\n",
        "\n",
        "            # Compute loss and optimize the model\n",
        "            loss = F.smooth_l1_loss(current_q_values, target_q_values.view(-1, 1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Update target network\n",
        "        if episode % target_update == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon for exploration-exploitation trade-off\n",
        "    epsilon *= epsilon_decay\n",
        "    epsilon = max(0.01, epsilon)  # Ensure epsilon doesn't go below a minimum value\n",
        "\n",
        "    # Print episode information\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "a1S1H7kR4hJf",
        "outputId": "f667d69d-8977-4afa-f18f-c60cb37ccd1f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected Tensor as element 0 in argument 0, but got int",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9f8e1707cc40>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mnon_final_next_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mstate_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0maction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mreward_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got int"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the trained model\n",
        "test_episodes = 10\n",
        "for _ in range(test_episodes):\n",
        "    state = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
        "\n",
        "    total_reward = 0\n",
        "    while True:\n",
        "        # Select the best action according to the policy network\n",
        "        action = policy_net(state).max(1)[1].view(1, 1).item()\n",
        "\n",
        "        # Perform the selected action\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = torch.tensor(next_state, dtype=torch.float32).view(1, -1)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            print(f\"Test Episode, Total Reward: {total_reward}\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "2kueOHUg4jEV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}